{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":460,"status":"ok","timestamp":1716475857233,"user":{"displayName":"Tristan","userId":"15709710985441936674"},"user_tz":-120},"id":"xG2IYVSbkqOt","metadata":{}},"outputs":[],"source":["import os\n","import h5py\n","\n","import torch\n","from torch import nn\n","import torchvision\n","from torchvision.transforms import v2\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from pathlib import Path"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12455,"status":"ok","timestamp":1716479884453,"user":{"displayName":"Tristan","userId":"15709710985441936674"},"user_tz":-120},"id":"_a-GGZ797h1I","metadata":{},"outputId":"efc0db02-68fb-4bdf-fe3a-94ca6b3a7f8d"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-23 20:53:26.125299: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-23 20:53:27.510696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["#!pip install astronn equinox einops\n","\n","from astroNN.datasets import load_galaxy10\n","from astroNN.datasets.galaxy10 import galaxy10cls_lookup, galaxy10_confusion"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"X9xx2m1Ho85u","metadata":{}},"outputs":[],"source":["# Just to make the session somewhat determinate\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","set_seed(0)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1716471159239,"user":{"displayName":"Tristan","userId":"15709710985441936674"},"user_tz":-120},"id":"10FaFPFCpBSL","metadata":{},"outputId":"4a7228b4-062c-4ba0-8e66-ccdd6e574aba"},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is available: False\n","If you want, you might want to switch to a GPU-accelerated session!\n"]}],"source":["print(f\"CUDA is available: {torch.cuda.is_available()}\")\n","\n","if not torch.cuda.is_available():\n","    print(\"If you want, you might want to switch to a GPU-accelerated session!\")\n","    device = torch.device('cpu')\n","else:\n","    device = torch.device('cuda')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":86588,"status":"ok","timestamp":1716473611759,"user":{"displayName":"Tristan","userId":"15709710985441936674"},"user_tz":-120},"id":"6wyvAJpjpiAP","metadata":{},"outputId":"5c63d27c-a201-4b7f-8930-8e19c30849fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/tristan/.astroNN/datasets/Galaxy10_DECals.h5 was found!\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# To load images and labels (will download automatically at the first time)\n","# First time downloading location will be ~/.astroNN/datasets/\n","load_galaxy10()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksyL5fN4wZ0Q"},"outputs":[],"source":["useful_images = images[labels == 5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDfxTc7PwSEL"},"outputs":[],"source":["train_split = 0.6\n","valid_split = 0.2\n","\n","full_dataset = useful_images\n","\n","test_split = 1 - train_split - valid_split\n","\n","train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n","        full_dataset, [train_split, valid_split, test_split]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVwcV0Wk1rUQ"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=128, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1716475729425,"user":{"displayName":"Tristan","userId":"15709710985441936674"},"user_tz":-120},"id":"6PLyO5W21kH2","metadata":{}},"outputs":[],"source":["import fastprogress\n","\n","\n","def train(dataloader, optimizer, model, loss_fn, device, master_bar,\n","          transform_common=None, transform_input=None):\n","    \"\"\"Run one training epoch.\n","\n","    Args:\n","        dataloader (DataLoader): Torch DataLoader object to load data\n","        optimizer: Torch optimizer object\n","        model (nn.Module): Torch model to train\n","        loss_fn: Torch loss function\n","        device (torch.device): Torch device to use for training\n","        master_bar (fastprogress.master_bar): Will be iterated over for each\n","            epoch to draw batches and display training progress\n","        transform_common (function): Transform to apply to input and target\n","        transform_input (function): Transform to apply to the input for de-noising.\n","            By default, no transform is carried out\n","\n","    Returns:\n","        float: Mean loss of this epoch\n","    \"\"\"\n","    epoch_loss = []\n","\n","    for x, _ in fastprogress.progress_bar(dataloader, parent=master_bar):\n","        optimizer.zero_grad()\n","        model.train()\n","\n","        x = transform_common(x) if transform_common else x\n","        x_inp = transform_input(x) if transform_input else x\n","\n","        # Forward pass\n","        x = x.to(device)\n","        x_inp = x_inp.to(device)\n","        x_hat, mu, logvar = model(x_inp)\n","\n","        # Compute loss\n","        loss = loss_fn(x_hat, x, mu, logvar)\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # For plotting the train loss, save it for each sample\n","        epoch_loss.append(loss.item())\n","        master_bar.child.comment = f\"Train Loss: {epoch_loss[-1]:.3f}\"\n","\n","    # Return the mean loss and the accuracy of this epoch\n","    return np.mean(epoch_loss)\n","\n","\n","def validate(dataloader, model, loss_fn, device, master_bar,\n","             transform_common=None, transform_input=None):\n","    \"\"\"Compute loss on validation set.\n","\n","    Args:\n","        dataloader (DataLoader): Torch DataLoader object to load data\n","        model (nn.Module): Torch model to train\n","        loss_fn: Torch loss function\n","        device (torch.device): Torch device to use for training\n","        master_bar (fastprogress.master_bar): Will be iterated over to draw\n","            batches and show validation progress\n","        transform_common (function): Transform to apply to input and target\n","        transform_input (function): Transform to apply to the input for de-noising.\n","            By default, no transform is carried out\n","\n","    Returns:\n","        float: Mean loss on validation set\n","    \"\"\"\n","    epoch_loss = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for x, _ in fastprogress.progress_bar(dataloader, parent=master_bar):\n","            x = transform_common(x) if transform_common else x\n","\n","            x_inp = transform_input(x) if transform_input else x\n","\n","            # make a prediction on test set\n","            x = x.to(device)\n","            x_inp = x_inp.to(device)\n","            x_hat, mu, logvar = model(x_inp)\n","\n","            # Compute loss\n","            loss = loss_fn(x_hat, x, mu, logvar)\n","\n","            # For plotting the train loss, save it for each sample\n","            epoch_loss.append(loss.item())\n","            master_bar.child.comment = f\"Valid. Loss: {epoch_loss[-1]:.3f}\"\n","\n","    # Return the mean loss, the accuracy and the confusion matrix\n","    return np.mean(epoch_loss)\n","\n","\n","\n","\n","def train_model(model, optimizer, loss_function, device, num_epochs,\n","                train_dataloader, valid_dataloader,\n","                transform_common=None, transform_input=None):\n","    \"\"\"Run model training.\n","\n","    Args:\n","        model (nn.Module): Torch model to train\n","        optimizer: Torch optimizer object\n","        loss_fn: Torch loss function for training\n","        device (torch.device): Torch device to use for training\n","        num_epochs (int): Max. number of epochs to train\n","        train_dataloader (DataLoader): Torch DataLoader object to load the\n","            training data\n","        valid_dataloader (DataLoader): Torch DataLoader object to load the\n","            test data\n","        transform_common (function): Transform to apply to input and target\n","        transform_input (function): Transform to apply to the input for de-noising.\n","            By default, no transform is carried out\n","\n","    Returns:\n","        list, list: Return list of train losses, test losses.\n","    \"\"\"\n","    master_bar = fastprogress.master_bar(range(num_epochs))\n","    epoch_list, train_losses, valid_losses = [], [], []\n","\n","    master_bar.names = [\"Train\", \"Valid.\"]\n","\n","    for epoch in master_bar:\n","        # Train the model\n","        epoch_train_loss = train(train_dataloader, optimizer, model, loss_function, device, master_bar, transform_common, transform_input)\n","        # Validate the model\n","        epoch_valid_loss = validate(valid_dataloader, model, loss_function, device, master_bar, transform_common, transform_input)\n","\n","        # Save loss and acc for plotting\n","        epoch_list.append(epoch + 1)\n","        train_losses.append(epoch_train_loss)\n","        valid_losses.append(epoch_valid_loss)\n","\n","        graphs = [[epoch_list, train_losses], [epoch_list, valid_losses]]\n","        x_bounds = [1, num_epochs]\n","\n","        master_bar.write(\n","            f\"Epoch {epoch + 1}, \"\n","            f\"avg. train loss: {epoch_train_loss:.3f}, \"\n","            f\"avg. valid. loss: {epoch_valid_loss:.3f}\"\n","        )\n","        master_bar.update_graph(graphs, x_bounds)\n","\n","\n","    return train_losses, valid_losses"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716477392543,"user":{"displayName":"Tristan","userId":"15709710985441936674"},"user_tz":-120},"id":"7djBzm896ATK","metadata":{}},"outputs":[],"source":["class Autoencoder(nn.Module):\n","\n","    def __init__(self, image_size=64,num_channels=3, latent_dims=128, num_filters=32, do_sampling=False):\n","        super(Autoencoder, self).__init__()\n","\n","        self.latent_dims  = latent_dims\n","        self.image_size   = image_size\n","        self.num_channels = num_channels\n","        self.num_filters  = num_filters\n","        self.do_sampling  = do_sampling\n","\n","        # Encoder\n","        self.conv_encoder = nn.Sequential(\n","            # TODO: Build the convolutional layers (torch.nn.Conv2d) here\n","            torch.nn.Conv2d(self.num_channels, self.num_channels, (4,4), 2, 1),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(self.num_channels,self.num_channels, (4,4), 2, 1),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(self.num_channels,self.num_channels, (4,4), 2, 1),\n","            torch.nn.ReLU(),\n","        )\n","\n","        # Linear Encoder\n","        # TODO: Match the dimensionality of the first and last layer here!\n","        self.fc_lin_down = nn.Linear(64*self.num_filters, 8 * self.num_filters)\n","        self.fc_mu       = nn.Linear(8 * self.num_filters, self.latent_dims)\n","        self.fc_logvar   = nn.Linear(self.latent_dims, self.latent_dims)\n","        self.fc_z        = nn.Linear(self.latent_dims, 8 * self.num_filters)\n","        self.fc_lin_up   = nn.Linear(8 * self.num_filters, 64*self.num_filters)\n","\n","        # Decoder\n","        self.conv_decoder = nn.Sequential(\n","            # TODO: Implement the reverse of the encoder here using torch.nn.ConvTranspose2d layers\n","            # The last activation here should be a sigmoid to keep the pixel values clipped in [0, 1)\n","            torch.nn.Conv2d(self.num_channels, self.num_channels, (4,4), 2, 1),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(self.num_channels,self.num_channels, (4,4), 2, 1),\n","            torch.nn.ReLU(),\n","            torch.nn.Conv2d(self.num_channels,self.num_channels, (4,4), 2, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def encode(self, x):\n","        ''' Encoder: output is (mean, log(variance))'''\n","        x       = self.conv_encoder(x)\n","        # Here, we resize the convolutional output appropriately for a linear layer\n","        # TODO: Fill in the correct dimensionality for the reordering\n","        x       = x.view(-1, self.num_filters * 8 * 8)\n","        x       = self.fc_lin_down(x)\n","        x       = nn.functional.relu(x)\n","        mu      = self.fc_mu(x)\n","        logvar  = self.fc_logvar(x)\n","        return mu, logvar\n","\n","    def sample(self, mu, logvar):\n","        ''' Sample from Gaussian with mean `mu` and SD `sqrt(exp(logvarz))`'''\n","        # Only use the full mean/stddev procedure if we want to later do sampling\n","        # And only reparametrise if we are in training mode\n","        if self.training and self.do_sampling:\n","            std = torch.exp(logvar * 0.5)\n","            eps = torch.randn_like(std)\n","            sample = mu + (eps * std)\n","            return sample\n","        else:\n","            return mu\n","\n","    def decode(self, z):\n","        '''Decoder: produces reconstruction from sample of latent z'''\n","        z = self.fc_z(z)\n","        z = nn.functional.relu(z)\n","        z = self.fc_lin_up(z)\n","        z = nn.functional.relu(z)\n","        # TODO: Fill in the correct dimensionality for the reordering here again\n","        z = z.view(-1, self.num_filters, 8, 8)\n","        z = self.conv_decoder(z)\n","        return z\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.sample(mu, logvar)\n","        x_hat = self.decode(z)\n","        if self.do_sampling:\n","            return x_hat, mu, logvar\n","        else:\n","            return x_hat, None, None"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1716477394985,"user":{"displayName":"Tristan","userId":"15709710985441936674"},"user_tz":-120},"id":"QTZOfcLu_bR1"},"outputs":[],"source":["def autoencoder_loss(recon_x, x, mu=None, logvar=None):\n","    mse_loss = torch.nn.functional.mse_loss(recon_x, x, reduction='sum') / x.size(dim=0)\n","\n","    if mu is not None and logvar is not None:\n","        raise NotImplementedError(\"Looks like you still need to implement the KL divergence loss!\")\n","    else:\n","        return mse_loss"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":259,"status":"ok","timestamp":1716477397033,"user":{"displayName":"Tristan","userId":"15709710985441936674"},"user_tz":-120},"id":"B1AVTs0f_gFv","metadata":{}},"outputs":[],"source":["model = Autoencoder()\n","torch.save(model,Path('model1'))\n","model_test = torch.load(Path('model1'))\n"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1716477399005,"user":{"displayName":"Tristan","userId":"15709710985441936674"},"user_tz":-120},"id":"VJVZD4r7AAOl","metadata":{},"outputId":"d185d5b4-5a78-4c14-b226-f3bd69be8725"},"outputs":[{"data":{"text/plain":["Autoencoder(\n","  (conv_encoder): Sequential(\n","    (0): Conv2d(3, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(3, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (3): ReLU()\n","    (4): Conv2d(3, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (5): ReLU()\n","  )\n","  (fc_lin_down): Linear(in_features=2048, out_features=256, bias=True)\n","  (fc_mu): Linear(in_features=256, out_features=128, bias=True)\n","  (fc_logvar): Linear(in_features=128, out_features=128, bias=True)\n","  (fc_z): Linear(in_features=128, out_features=256, bias=True)\n","  (fc_lin_up): Linear(in_features=256, out_features=2048, bias=True)\n","  (conv_decoder): Sequential(\n","    (0): Conv2d(3, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): ReLU()\n","    (2): Conv2d(3, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (3): ReLU()\n","    (4): Conv2d(3, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (5): Sigmoid()\n","  )\n",")"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["model_test"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
